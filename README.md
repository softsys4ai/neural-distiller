# PaKD Model Compression
<b>Project:</b> Multi-stage Compression of Deep Neural Networks through Pruning and Knowledge Distillation<br>
<b>Contributers:</b> Blake Edwards<br>
<b>References:</b> Dr. Pooyan Jamshidi<br>
<br><b>Description:</b> <br>
Almost everyone today uses a smart mobile phone that they carry with them at all times. The ability to run powerful artificial intelligence (AI) technologies on our phones would revolutionize the world and equip mobile devices with an increasing amount of functionality. However, we do not want AI technologies to drain our battery and require us to charge our phones multiple times per day in order to run them. Our project will reduce the work needed to run AI technologies and enable them to be run on devices constrained by battery, power, and storage, like our mobile phones.
<br><br><b>Significance:</b><br>
Our project is significant because it will allow large neural networks, that achieve state-of-the-art accuracy in a variety of different fields including, but not limited to, computer vision, natural language processing, and speech recognition, to be run on small devices [7, 13]. Neural networks perform the best on these tasks but in order for them to become more accurate they grow in size, thus becoming more expensive to run. Our project intends to combat this issue by compressing neural networks through the removal of unnecessary components. This is important because it will allow them to run on smaller devices with less computational power, storage, and memory bandwidth. This is crucial because if we do not compress large neural networks, we will not be able to use them in real world applications because not everyone has access to a supercomputer.
<br>

<b>Goal:</b><br>
The goal of this project is to publish the compression results of the PaKD model compression technique, develop our code base to generically compress deep neural networks, compress a neural network by at least 11 times and increase its speed by an average of 4 times [4].